{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "60bc7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e3b0e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing data\n",
    "\n",
    "# (1)\n",
    "# Corporate giving dataset - local\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# Dropping irrelevant columns (those not in data dictionary) from corporate philanthropy dataframe\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Text data - local\n",
    "text_df = pd.read_excel(\"ENVS_documents_for_text_analysis.xlsx\")\n",
    "text_df.drop(columns={'Researcher', 'Word_counts'}, inplace = True)\n",
    "\n",
    "\n",
    "# (3)\n",
    "# Discourses of Delay - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1MhB60vzde7KT9Ti6eQtimmWvYAEersI4zK3L_gwDNA8/edit#gid=0\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "delay_df = pd.read_csv(spreadsheet_url, header=0)\n",
    "\n",
    "simple_delay_names = {\"Individualism\": \"Individualism\", \"Whataboutism\":\n",
    "             \"Whataboutism\", \"Doomism\": \"Doomism\",\n",
    "             \"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots',\n",
    "             \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being',\n",
    "             \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice',\n",
    "             \"Change is impossible\": 'Change_impossible'}\n",
    "\n",
    "# Converting dataframe to dictionary with DoD words in list format\n",
    "complete_discourse_dict = {}\n",
    "for row in delay_df.iterrows():\n",
    "    delay_method = row[1][\"Sub-category\"]\n",
    "    dict_words = row[1][\"Current_dict\"].split(\", \")\n",
    "    complete_discourse_dict[simple_delay_names[delay_method]] = dict_words\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "discourse_dict = complete_discourse_dict.copy()\n",
    "\n",
    "# Pulling tech_optimism\n",
    "techOptimism = discourse_dict['Tech_optimism']\n",
    "\n",
    "# (4)\n",
    "# Importing annualized_donations - local\n",
    "annualized_donations_df = pd.read_excel('annualized_donations.xlsx')\n",
    "\n",
    "# (5)\n",
    "# Making simple group names\n",
    "simpleGroupNames = {\"american forests\": \"af\", \"world wildlife fund\": \"wwf\",\n",
    "                     \"sierra club\": 'sc', \"nature conservancy\": \"tnc\",\n",
    "                     \"natural resources defense council\": \"nrdc\",\n",
    "                     \"national fish and wildlife foundation\": \"nfwf\",\n",
    "                     \"environmental defense fund\": \"edf\",\n",
    "                     \"conservation international\": \"ci\",\n",
    "                     \"audubon society\": \"nas\", \"ocean conservancy\": \"oc\",\n",
    "                     \"earthjustice\": \"ej\", 'greenpeace': 'gp'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "765365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data wrangling\n",
    "\n",
    "# (1)\n",
    "# Text cleaning\n",
    "# Importing punctuation and stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’') \n",
    "\n",
    "# Cleaning text data\n",
    "textCleaned = [] # Creating an empty list to store list of cleaned words\n",
    "for row in text_df[\"Document_text\"]: # Looping through each Tweet in ukraineRussia_df\n",
    "    rowCleaned = [] # Creating an empty list to store cleaned words from each Tweet\n",
    "    row_as_list = str(row).split() # Splitting row into a list of words at ' '\n",
    "    for word in row_as_list: # Looping through each word in row_as_list\n",
    "        if word not in stopWords and word != \"nan\":\n",
    "            text = word.translate(table_punctuation) # Translating punctuation into ''\n",
    "            textLower = text.lower() # Converting text to lowercase\n",
    "            rowCleaned.append(textLower) # Appending cleaned word to rowCleaned list\n",
    "    textCleaned.append(rowCleaned)  # Appending rowCleaned to textCleaned list\n",
    "text_df[\"cleaned_text\"] = textCleaned\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating document-term matrix\n",
    "delay_types = ['Tech_optimism']\n",
    "\n",
    "# Creating copy of text_df to work with\n",
    "regression_df = text_df.copy()\n",
    "    \n",
    "# Creating DoD_results dict\n",
    "DoD_results = {}\n",
    "\n",
    "wordsInText = []\n",
    "# Looping through each DoD word\n",
    "for col in techOptimism:\n",
    "    wordAppearance = []\n",
    "    # Looping through each entry in text_df[\"cleaned_text\"]\n",
    "    for text in text_df[\"cleaned_text\"]:\n",
    "        mySum = 0\n",
    "        prevWord = \"\"\n",
    "        for word in text:\n",
    "            # Incrementing if word in DoD vocabulary\n",
    "            if word == col:\n",
    "                mySum += 1\n",
    "            bigram = prevWord + \" \" + word\n",
    "            # Incrementing if bigram in DoD vocabulary\n",
    "            if bigram == col:\n",
    "                mySum += 1\n",
    "            prevWord = word  \n",
    "        wordAppearance.append(mySum)\n",
    "    # Adding word to regression_df if it appears in corpus\n",
    "    if (sum(wordAppearance) > 0):\n",
    "        regression_df[col] = wordAppearance\n",
    "        regression_df = regression_df.copy()\n",
    "        wordsInText.append(col)\n",
    "\n",
    "# (3)\n",
    "# Adding donation information to regression_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Organization_name\", \"year\": \"Document_year\"}) # Renaming group column\n",
    "regression_df[\"Organization_name\"] = regression_df[\"Organization_name\"].str.lower() # Converting name to lowercase\n",
    "regression_df = pd.merge(regression_df, annualized_donations_df, on = [\n",
    "    'Organization_name', 'Document_year'], how = 'outer') # Mergining on organization name and document year\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(0)\n",
    "\n",
    "\n",
    "# (4)\n",
    "# Computing word counts for each entry\n",
    "wordCounts = []\n",
    "for entry in regression_df[\"cleaned_text\"]:\n",
    "    wordCounts.append(len(str(entry).split()))\n",
    "regression_df[\"Word_counts\"] = wordCounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8f8f4cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:           grant_amount   R-squared (uncentered):                   0.088\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.088\n",
      "Method:                 Least Squares   F-statistic:                              265.5\n",
      "Date:                Sun, 15 May 2022   Prob (F-statistic):                   8.96e-111\n",
      "Time:                        10:40:17   Log-Likelihood:                         -77963.\n",
      "No. Observations:                5478   AIC:                                  1.559e+05\n",
      "Df Residuals:                    5476   BIC:                                  1.559e+05\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "techOpt_frequency  6.969e+04    1.5e+04      4.659      0.000    4.04e+04     9.9e+04\n",
      "Document_year        52.5679      2.571     20.445      0.000      47.527      57.608\n",
      "==============================================================================\n",
      "Omnibus:                     5399.745   Durbin-Watson:                   0.038\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           213735.360\n",
      "Skew:                           4.984   Prob(JB):                         0.00\n",
      "Kurtosis:                      31.932   Cond. No.                     6.05e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The condition number is large, 6.05e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Regression with dictionary counts as independent variables\n",
    "\n",
    "# Making frequency matrix\n",
    "wordCounts = regression_df[\"Word_counts\"]\n",
    "\n",
    "# Creating variables data\n",
    "variables = wordsInText.copy()\n",
    "variables = set(variables + ['grant_amount', 'Word_counts', 'Document_year', 'Organization_name'])\n",
    "\n",
    "# Creating techOptimsm frequency measure\n",
    "reg_df = regression_df[variables]\n",
    "reg_df['techOpt_frequency'] = [0] * len(reg_df['grant_amount'])\n",
    "for word in wordsInText:\n",
    "    reg_df['techOpt_frequency'] += reg_df[word]\n",
    "reg_df['techOpt_frequency'] = reg_df['techOpt_frequency'] / wordCounts * 100\n",
    "    \n",
    "# Dropping null values\n",
    "reg_df.dropna(inplace = True)\n",
    "\n",
    "# # Adding group controls\n",
    "# groupList = list(reg_df['Organization_name'].unique())\n",
    "# groupList.remove('nature conservancy')\n",
    "\n",
    "# ivList = []\n",
    "# for group in groupList:\n",
    "#     iv = simpleGroupNames[group] + '_indc'\n",
    "#     reg_df[iv] = np.where(reg_df['Organization_name'].str.contains(group), 1, 0)\n",
    "#     ivList.append(iv)\n",
    "\n",
    "# # Dropping greenpeace and earthjustice\n",
    "# reg_df = reg_df[~reg_df['Organization_name'].isin(['greenpeace', 'earthjustice'])]\n",
    "    \n",
    "# Splitting data into X and Y\n",
    "y = reg_df['grant_amount']\n",
    "X = reg_df[['techOpt_frequency', 'Document_year']]\n",
    "\n",
    "# Creating linear regression\n",
    "olsReg = sm.OLS(y, X).fit()\n",
    "print(olsReg.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6a0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
