{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452ee96c",
   "metadata": {},
   "source": [
    "# Philanthropy Regressions\n",
    "### Finnian Lowden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1424f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data dictionaries\n",
    "\n",
    "\n",
    "# donations_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Grantmaker_name             String     Corporation/foundation that gave grant\n",
    "# Year                        Int        Year grant was given\n",
    "# Recipient_name              String     Organization that recived grant\n",
    "# NTEE_code                   String     NTEE code of organization given grant\n",
    "# NTEE_category               String     Broader category of organization according to IRS\n",
    "# Grant Amount                Float      Grant amount adjusted for inflation to 2020 dollars\n",
    "# Recipient_city              String     City of recipient organization\n",
    "# Recipient_state             String     State of recipient organization\n",
    "\n",
    "\n",
    "# text_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Group                       String     Name of environmental nonprofit\n",
    "# Individualism               Int        Measure of prevalence of this discourse of delay (DoD) in the text\n",
    "# The 'free rider' excuse     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Whataboutism                Int        Measure of prevalence of this DoD in the text in given year\n",
    "# All talk, little action     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Fossil fuel solutionism     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# No sticks, just carrots     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Technological optimism      Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to well-being        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Policy perfectionism        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to social justice    Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Change is impossible        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Doomism                     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Year                        Int        Year associated with prevalence measure\n",
    "# gp_indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ccb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/finnianlowden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c46f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n"
     ]
    }
   ],
   "source": [
    "### Importing data\n",
    "\n",
    "# Corporate giving dataset\n",
    "# The dataset is large, so it takes a little while\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# Text analysis results dataset\n",
    "complete_text_df = pd.read_excel(\"DoD_results.xlsx\")\n",
    "\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b659f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Formatting dataframes\n",
    "\n",
    "# Working with corporate philanthropy data\n",
    "# Dropping irrelevant columns (those not in data dictionary)\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "text_df = complete_text_df.copy()\n",
    "\n",
    "# Checking to make sure changes were made\n",
    "# donations_df.head()\n",
    "# text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8cbd25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Individualism</th>\n",
       "      <th>Free_rider</th>\n",
       "      <th>Whataboutism</th>\n",
       "      <th>Talk_no_action</th>\n",
       "      <th>FF_solutionism</th>\n",
       "      <th>Carrots</th>\n",
       "      <th>Tech_optimism</th>\n",
       "      <th>Well_being</th>\n",
       "      <th>Perfect_policy</th>\n",
       "      <th>...</th>\n",
       "      <th>Doomism_control</th>\n",
       "      <th>AF_indc</th>\n",
       "      <th>NFWF_indc</th>\n",
       "      <th>NRDC_indc</th>\n",
       "      <th>CI_indc</th>\n",
       "      <th>WWF_indc</th>\n",
       "      <th>SC_indc</th>\n",
       "      <th>OC_indc</th>\n",
       "      <th>EDF_indc</th>\n",
       "      <th>NAS_indc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nature conservancy</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american forests</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>national fish and wildlife foundation</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural resources defense council</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conservation international</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Group  Individualism  Free_rider  \\\n",
       "0                     nature conservancy       0.004436    0.000000   \n",
       "1                       american forests     999.000000  999.000000   \n",
       "2  national fish and wildlife foundation     999.000000  999.000000   \n",
       "3      natural resources defense council       0.018817    0.000159   \n",
       "4             conservation international     999.000000  999.000000   \n",
       "\n",
       "   Whataboutism  Talk_no_action  FF_solutionism     Carrots  Tech_optimism  \\\n",
       "0      0.000127        0.000760             0.0    0.000634       0.000380   \n",
       "1    999.000000      999.000000           999.0  999.000000     999.000000   \n",
       "2    999.000000      999.000000           999.0  999.000000     999.000000   \n",
       "3      0.000000        0.001595             0.0    0.000000       0.000159   \n",
       "4    999.000000      999.000000           999.0  999.000000     999.000000   \n",
       "\n",
       "   Well_being  Perfect_policy  ...  Doomism_control  AF_indc  NFWF_indc  \\\n",
       "0    0.000000        0.000127  ...            999.0        0          0   \n",
       "1  999.000000      999.000000  ...            999.0        1          0   \n",
       "2  999.000000      999.000000  ...            999.0        0          1   \n",
       "3    0.000797        0.000000  ...            999.0        0          0   \n",
       "4  999.000000      999.000000  ...            999.0        0          0   \n",
       "\n",
       "   NRDC_indc  CI_indc  WWF_indc  SC_indc  OC_indc  EDF_indc  NAS_indc  \n",
       "0          0        0         0        0        0         0         0  \n",
       "1          0        0         0        0        0         0         0  \n",
       "2          0        0         0        0        0         0         0  \n",
       "3          1        0         0        0        0         0         0  \n",
       "4          0        1         0        0        0         0         0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Processing data for regressions\n",
    "# (1)\n",
    "# Adding corporate giving amount for each nonprofit to text_df\n",
    "reduce_donations_df = donations_df.copy()\n",
    "group_list = ['nature conservancy', 'american forests', 'national fish and wildlife foundation',\n",
    " 'natural resources defense council', 'conservation international', 'world wildlife fund',\n",
    " 'sierra club', 'ocean conservancy', 'environmental defense fund', 'audubon society']\n",
    "reduce_donations_df[\"recipient_name\"] = reduce_donations_df[\"recipient_name\"].str.lower()\n",
    "boolean_series = reduce_donations_df[\"recipient_name\"].isin(group_list)\n",
    "reduce_donations_df = reduce_donations_df[boolean_series]\n",
    "\n",
    "# Grouping by year and group\n",
    "annualized_donations_df = reduce_donations_df.groupby(\n",
    "    ['recipient_name', 'year'], as_index = False).agg({'grant_amount': sum})\n",
    "annualized_donations_df = pd.DataFrame(annualized_donations_df)\n",
    "# annualized_donations_df.to_excel(\"Output.xlsx\", index = False) # code to download as XSLX\n",
    "\n",
    "# Adding corporate giving to regression_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Group\", \"year\": \"Year\"}) # Renaming group column\n",
    "text_df[\"Group\"] = text_df[\"Group\"].str.lower()\n",
    "regression_df = pd.merge(text_df, annualized_donations_df, on = ['Group', 'Year'], how = 'outer')\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(999)\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Formatting control data in regression_df\n",
    "control_prevalence_df = text_df.copy()\n",
    "control_list = ['greenpeace', 'earthjustice']\n",
    "control_prevalence_df[\"Group\"] = control_prevalence_df[\"Group\"].str.lower()\n",
    "boolean_series = control_prevalence_df[\"Group\"].isin(control_list)\n",
    "control_prevalence_df = control_prevalence_df[boolean_series]\n",
    "\n",
    "# Grouping prevalence by year\n",
    "annualized_control_prevalence_df = control_prevalence_df.groupby(\n",
    "    ['Year'], as_index = False).agg({\"Individualism\": 'sum', \"The 'free rider' excuse\": 'sum',\n",
    "                                     \"Whataboutism\": 'sum', \"All talk, little action\": 'sum',\n",
    "                                     \"Fossil fuel solutionism\": 'sum', \"No sticks, just carrots\": 'sum',\n",
    "                                     \"Technological optimism\": 'sum', \"Appeal to well-being\": 'sum',\n",
    "                                     \"Policy perfectionism\": 'sum', \"Appeal to social justice\": 'sum',\n",
    "                                     \"Change is impossible\": 'sum', \"Doomism\": 'sum'})\n",
    "annualized_control_prevalence_df = pd.DataFrame(annualized_control_prevalence_df)\n",
    "\n",
    "# Calculating mean\n",
    "column_headers = list(annualized_control_prevalence_df)\n",
    "column_headers.remove(\"Year\")\n",
    "for column in column_headers:\n",
    "    count = 0\n",
    "    for value in annualized_control_prevalence_df[column]:\n",
    "        # If value is less than 999 -> data for both groups, so divide sum by 2\n",
    "        if value < 999:\n",
    "            annualized_control_prevalence_df.at[count, column] = value / 2\n",
    "        # If value is less than 1998 and greater than or equal to 999 -> data for one group, so divide sum by 1\n",
    "        elif value >= 999 and value < 1998:\n",
    "            annualized_control_prevalence_df.at[count, column] = value - 999\n",
    "        # If value is equal to 1998 -> no data for either group\n",
    "        else:\n",
    "            annualized_control_prevalence_df.at[count, column] = 999\n",
    "        count += 1\n",
    "        \n",
    "annualized_control_prevalence_df = annualized_control_prevalence_df.rename(\n",
    "    columns={\"Individualism\": 'Individualism_control', \"The 'free rider' excuse\": 'Free_rider_control',\n",
    "             \"Whataboutism\": 'Whataboutism_control', \"All talk, little action\": 'Talk_no_action_control',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism_control', \"No sticks, just carrots\": 'Carrots_control',\n",
    "             \"Technological optimism\": 'Tech_optimism_control', \"Appeal to well-being\": 'Well_being_control',\n",
    "             \"Policy perfectionism\": 'Perfect_policy_control', \"Appeal to social justice\": 'Social_justice_control',\n",
    "             \"Change is impossible\": 'Change_impossible_control', \"Doomism\": 'Doomism_control'})\n",
    "\n",
    "# Adding control data to regression_df\n",
    "regression_df = pd.merge(regression_df, annualized_control_prevalence_df, on = ['Year'], how = 'outer')\n",
    "\n",
    "\n",
    "# (3)\n",
    "# Adding indicator variables for each group\n",
    "text_df['TNC'] = np.where(text_df['Group'] == 'Nature Conservancy', 1, 0) # Not including to avoid perfect multico\n",
    "regression_df['AF_indc'] = np.where(regression_df['Group'] == 'american forests', 1, 0)\n",
    "regression_df['NFWF_indc'] = np.where(regression_df['Group'] == 'national fish and wildlife foundation', 1, 0)\n",
    "regression_df['NRDC_indc'] = np.where(regression_df['Group'] == 'natural resources defense council', 1, 0)\n",
    "regression_df['CI_indc'] = np.where(regression_df['Group'] == 'conservation international', 1, 0)\n",
    "regression_df['WWF_indc'] = np.where(regression_df['Group'] == 'world wildlife fund', 1, 0)\n",
    "regression_df['SC_indc'] = np.where(regression_df['Group'] == 'sierra club', 1, 0)\n",
    "regression_df['OC_indc'] = np.where(regression_df['Group'] == 'ocean conservancy', 1, 0)\n",
    "regression_df['EDF_indc'] = np.where(regression_df['Group'] == 'environmental defense fund', 1, 0)\n",
    "regression_df['NAS_indc'] = np.where(regression_df['Group'] == 'audubon society', 1, 0)\n",
    "\n",
    "# Renaming columns in regression_df\n",
    "regression_df = regression_df.rename(columns={\"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action', \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots', \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being', \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice', \"Change is impossible\": 'Change_impossible'})\n",
    "\n",
    "# (4)\n",
    "# Saving regression_df as excel file\n",
    "data_df = regression_df.copy()\n",
    "regression_df.to_excel(\"regression_df.xlsx\", index=False)\n",
    "\n",
    "# (5)\n",
    "# Checking to make sure changes were made\n",
    "regression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20462583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "                        Linear\n",
      "------------------------------\n",
      "grant_amount           0.00***\n",
      "                       (0.00) \n",
      "Talk_no_action_control 0.62***\n",
      "                       (0.04) \n",
      "AF_indc                0.00   \n",
      "                       (0.00) \n",
      "NFWF_indc              0.00   \n",
      "                       (0.00) \n",
      "NRDC_indc              -0.00  \n",
      "                       (0.00) \n",
      "CI_indc                0.00***\n",
      "                       (0.00) \n",
      "WWF_indc               0.00***\n",
      "                       (0.00) \n",
      "SC_indc                0.00** \n",
      "                       (0.00) \n",
      "OC_indc                0.00***\n",
      "                       (0.00) \n",
      "EDF_indc               -0.00**\n",
      "                       (0.00) \n",
      "NAS_indc               0.00   \n",
      "                       (0.00) \n",
      "R-squared              0.73   \n",
      "R-squared Adj.         0.72   \n",
      "N                      492    \n",
      "==============================\n",
      "Standard errors in\n",
      "parentheses.\n",
      "* p<.1, ** p<.05, ***p<.01\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:         Talk_no_action   R-squared (uncentered):                   0.730\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.724\n",
      "Method:                 Least Squares   F-statistic:                              118.3\n",
      "Date:                Wed, 09 Mar 2022   Prob (F-statistic):                   4.27e-129\n",
      "Time:                        18:15:11   Log-Likelihood:                          2695.3\n",
      "No. Observations:                 492   AIC:                                     -5369.\n",
      "Df Residuals:                     481   BIC:                                     -5322.\n",
      "Df Model:                          11                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "grant_amount            4.084e-10   1.32e-10      3.085      0.002    1.48e-10    6.68e-10\n",
      "Talk_no_action_control     0.6226      0.041     15.175      0.000       0.542       0.703\n",
      "AF_indc                    0.0001      0.000      0.799      0.425      -0.000       0.000\n",
      "NFWF_indc                  0.0001      0.000      0.532      0.595      -0.000       0.001\n",
      "NRDC_indc              -5.595e-05      0.000     -0.318      0.751      -0.000       0.000\n",
      "CI_indc                    0.0013      0.000      7.242      0.000       0.001       0.002\n",
      "WWF_indc                   0.0007      0.000      3.851      0.000       0.000       0.001\n",
      "SC_indc                    0.0004      0.000      2.521      0.012    9.77e-05       0.001\n",
      "OC_indc                    0.0006      0.000      3.358      0.001       0.000       0.001\n",
      "EDF_indc                  -0.0005      0.000     -2.558      0.011      -0.001      -0.000\n",
      "NAS_indc                   0.0003      0.000      1.539      0.124   -7.47e-05       0.001\n",
      "==============================================================================\n",
      "Omnibus:                       91.763   Durbin-Watson:                   1.842\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              204.332\n",
      "Skew:                           0.981   Prob(JB):                     4.26e-45\n",
      "Kurtosis:                       5.473   Cond. No.                     4.54e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The condition number is large, 4.54e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "                       var       val_ols\n",
      "0             grant_amount  4.083585e-10\n",
      "1   Talk_no_action_control  6.226399e-01\n",
      "2                  AF_indc  1.415080e-04\n",
      "3                NFWF_indc  1.074331e-04\n",
      "4                NRDC_indc -5.594850e-05\n",
      "5                  CI_indc  1.277850e-03\n",
      "6                 WWF_indc  6.765985e-04\n",
      "7                  SC_indc  4.431490e-04\n",
      "8                  OC_indc  5.922571e-04\n",
      "9                 EDF_indc -4.510965e-04\n",
      "10                NAS_indc  2.699103e-04\n"
     ]
    }
   ],
   "source": [
    "### Regression work\n",
    "regression_df = data_df.copy()\n",
    "y_list = ['Individualism', 'Free_rider', 'Whataboutism', 'Talk_no_action', 'FF_solutionism',\n",
    "'Carrots', 'Tech_optimism', 'Well_being', 'Perfect_policy', 'Social_justice',\n",
    "'Change_impossible', 'Doomism']\n",
    "\n",
    "# Selecting current y\n",
    "current_y = 'Talk_no_action' # Select whatever Y I want to see (e.g., Individualism prevalence)\n",
    "\n",
    "# Dropping irrelevant ys and controls\n",
    "control_list = [x + \"_control\" for x in y_list]\n",
    "current_control = current_y + \"_control\"\n",
    "control_list.remove(current_control)\n",
    "y_list = y_list + control_list\n",
    "\n",
    "### Dealing with missing values\n",
    "\n",
    "### METHOD 1: Using averages\n",
    "imp = SimpleImputer(missing_values=999, strategy='mean')\n",
    "group_series = regression_df[\"Group\"]\n",
    "regression_df.drop(columns=[\"Group\"], inplace=True)\n",
    "column_headers = list(regression_df)\n",
    "imp = imp.fit_transform(regression_df)\n",
    "regression_df = pd.DataFrame(imp, columns=column_headers)\n",
    "regression_df[\"Group\"] = group_series\n",
    "\n",
    "# Creating X and Y data from regression_df\n",
    "y = regression_df[current_y]\n",
    "X = regression_df.drop(columns = (y_list + [current_y, \"Year\", \"Group\"]), axis=1)\n",
    "\n",
    "### Linear regression work\n",
    "olsReg = sm.OLS(y, X).fit()\n",
    "listResults = [olsReg]\n",
    "modelNames = ['Linear']\n",
    "print(summary_col(listResults, stars = True, float_format = '%0.2f', regressor_order = ['x1'],\n",
    "                  info_dict = {'N': lambda x: \"{0:d}\".format(int(x.nobs))}, model_names = modelNames))\n",
    "\n",
    "# Printing results from reg\n",
    "print(olsReg.summary())\n",
    "coef_comp = pd.DataFrame({'var': X.columns, 'val_ols': olsReg.params.tolist()})\n",
    "print(\"\\n\" + str(coef_comp))\n",
    "\n",
    "\n",
    "# ### METHOD 2: Dropping null values\n",
    "# regression_df = regression_df[regression_df[\"grant_amount\"] != 999]\n",
    "# regression_df = regression_df[regression_df[current_control]!=999]\n",
    "# regression_df = regression_df[regression_df[current_y]!=999]\n",
    "# regression_df = regression_df[[current_y, current_control, \"grant_amount\"]].diff()\n",
    "# regression_df.dropna(inplace=True)\n",
    "\n",
    "# # Creating X and Y data from regression_df\n",
    "# y = regression_df[current_y]\n",
    "# X = regression_df.drop(columns=[current_y])\n",
    "\n",
    "# ### Linear regression work\n",
    "# olsReg = sm.OLS(y, X).fit()\n",
    "# listResults = [olsReg]\n",
    "# modelNames = ['Linear']\n",
    "# print(summary_col(listResults, stars = True, float_format = '%0.2f', regressor_order = ['x1'],\n",
    "#                   info_dict = {'N': lambda x: \"{0:d}\".format(int(x.nobs))}, model_names = modelNames))\n",
    "\n",
    "# # Printing results from reg\n",
    "# print(olsReg.summary())\n",
    "# coef_comp = pd.DataFrame({'var': X.columns, 'val_ols': olsReg.params.tolist()})\n",
    "# print(\"\\n\" + str(coef_comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b527e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
