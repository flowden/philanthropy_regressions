{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cda3a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/finnianlowden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dfa8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data dictionaries\n",
    "\n",
    "\n",
    "# donations_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Grantmaker_name             String     Corporation/foundation that gave grant\n",
    "# Year                        Int        Year grant was given\n",
    "# Recipient_name              String     Organization that recived grant\n",
    "# NTEE_code                   String     NTEE code of organization given grant\n",
    "# NTEE_category               String     Broader category of organization according to IRS\n",
    "# Grant Amount                Float      Grant amount adjusted for inflation to 2020 dollars\n",
    "# Recipient_city              String     City of recipient organization\n",
    "# Recipient_state             String     State of recipient organization\n",
    "\n",
    "\n",
    "# text_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Group                       String     Name of environmental nonprofit\n",
    "# Individualism               Float      Measure of prevalence of this discourse of delay (DoD) in the text\n",
    "# The 'free rider' excuse     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Whataboutism                Float      Measure of prevalence of this DoD in the text in given year\n",
    "# All talk, little action     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Fossil fuel solutionism     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# No sticks, just carrots     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Technological optimism      Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to well-being        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Policy perfectionism        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to social justice    Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Change is impossible        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Doomism                     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Year                        Int        Year associated with prevalence measure\n",
    "# AF_indc                     Int        Indicator variable for American Forests (AF)\n",
    "# NFWF_indc                   Int        Indicator variable for the National Fish and Wildlife Foundation (NFWF)\n",
    "# NRDC_indc                   Int        Indicator variable for the Natural Resources Defense Council (NRDC)\n",
    "# CI_indc                     Int        Indicator variable for Conservation International(CI)\n",
    "# WWF_indc                    Int        Indicator variable for the World Wildlife Fund (WWF)\n",
    "# SC_indc                     Int        Indicator variable for the Sierra Club (SC)\n",
    "# OC_indc                     Int        Indicator variable for The Ocean Conservancy (OC)\n",
    "# EDF_indc                    Int        Indicator variable for the Environmental Defense Fund (EDF)\n",
    "# NAS_indc                    Int        Indicator variable for the National Audubon Society (NAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13cd86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing data\n",
    "\n",
    "# (1)\n",
    "# Corporate giving dataset - local\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# (2)\n",
    "# Text data\n",
    "# Adding experiment text data - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=570879331\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "experiment_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Adding control text data\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=1393581184\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "control_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Joining control and experiment\n",
    "complete_text_df = pd.concat([experiment_text_df, control_text_df])\n",
    "\n",
    "# (3)\n",
    "# Discourses of Delay - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1MhB60vzde7KT9Ti6eQtimmWvYAEersI4zK3L_gwDNA8/edit#gid=0\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "delay_df = pd.read_csv(spreadsheet_url, header=0)\n",
    "\n",
    "simple_delay_names = {\"Individualism\": \"Individualism\", \"Whataboutism\":\n",
    "             \"Whataboutism\", \"Doomism\": \"Doomism\",\n",
    "             \"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots',\n",
    "             \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being',\n",
    "             \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice',\n",
    "             \"Change is impossible\": 'Change_impossible'}\n",
    "\n",
    "complete_discourse_dict = {}\n",
    "for row in delay_df.iterrows():\n",
    "    delay_method = row[1][\"Sub-category\"]\n",
    "    dict_words = row[1][\"Current_dict\"].split(\", \")\n",
    "    complete_discourse_dict[simple_delay_names[delay_method]] = dict_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78b02c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Formatting dataframes\n",
    "\n",
    "# Working with corporate philanthropy data\n",
    "# Dropping irrelevant columns (those not in data dictionary)\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "text_df = complete_text_df.copy()\n",
    "text_df.drop(columns={'Researcher'}, inplace = True)\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "discourse_dict = complete_discourse_dict.copy()\n",
    "\n",
    "# Checking to make sure changes were made\n",
    "# donations_df.head()\n",
    "# text_df.head()\n",
    "# discourse_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d2ec1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/x813n1xn7r1f_k68nnt2v9t80000gn/T/ipykernel_32623/1083205813.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  regression_df[col] = wordAppearance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/x813n1xn7r1f_k68nnt2v9t80000gn/T/ipykernel_32623/1083205813.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  regression_df['recieved_donation'] = np.where(regression_df['grant_amount'] > 0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "### Data wrangling\n",
    "\n",
    "# (1)\n",
    "# Text cleaning\n",
    "# Importing punctuation and stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™') \n",
    "\n",
    "# Cleaning text data\n",
    "textCleaned = [] # Creating an empty list to store list of cleaned words\n",
    "for row in text_df[\"Document_text\"]: # Looping through each Tweet in ukraineRussia_df\n",
    "    rowCleaned = [] # Creating an empty list to store cleaned words from each Tweet\n",
    "    row_as_list = str(row).split() # Splitting row into a list of words at ' '\n",
    "    for word in row_as_list: # Looping through each word in row_as_list\n",
    "        if word not in stopWords and word != \"nan\":\n",
    "            text = word.translate(table_punctuation) # Translating punctuation into ''\n",
    "            textLower = text.lower() # Converting text to lowercase\n",
    "            rowCleaned.append(textLower) # Appending cleaned word to rowCleaned list\n",
    "    textCleaned.append(rowCleaned)  # Appending rowCleaned to textCleaned list\n",
    "\n",
    "text_df[\"cleaned_text\"] = textCleaned\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating document-term matrix\n",
    "# Getting list of Discourses of Delay\n",
    "# best_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\"]\n",
    "# top6_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\", \"Free_rider\", \"Whataboutism\"]\n",
    "all_dicts = list(discourse_dict)\n",
    "delay_types = all_dicts\n",
    "\n",
    "# Getting all words in DoD dictionaries\n",
    "delay_vocabulary = set()\n",
    "for delay in delay_types:\n",
    "    delay_vocabulary.update(discourse_dict[delay])\n",
    "    \n",
    "regression_df = text_df.copy()\n",
    "    \n",
    "for col in delay_vocabulary:\n",
    "    wordAppearance = []\n",
    "    for text in text_df[\"cleaned_text\"]:\n",
    "        mySum = 0\n",
    "        prevWord = \"\"\n",
    "        for word in text:\n",
    "            if word == col:\n",
    "                mySum += 1\n",
    "            bigram = prevWord + \" \" + word\n",
    "            if bigram == col:\n",
    "                mySum += 1\n",
    "            prevWord = word\n",
    "        wordAppearance.append(mySum)\n",
    "    if (sum(wordAppearance) > 0):\n",
    "        regression_df[col] = wordAppearance\n",
    "\n",
    "# (3)\n",
    "# Adding donation information\n",
    "# Adding amount of donation in given year\n",
    "reduce_donations_df = donations_df.copy()\n",
    "group_list = ['nature conservancy', 'american forests', 'national fish and wildlife foundation',\n",
    " 'natural resources defense council', 'conservation international', 'world wildlife fund',\n",
    " 'sierra club', 'ocean conservancy', 'environmental defense fund', 'audubon society']\n",
    "reduce_donations_df[\"recipient_name\"] = reduce_donations_df[\"recipient_name\"].str.lower()\n",
    "boolean_series = reduce_donations_df[\"recipient_name\"].isin(group_list)\n",
    "reduce_donations_df = reduce_donations_df[boolean_series]\n",
    "\n",
    "# Grouping by year and group\n",
    "annualized_donations_df = reduce_donations_df.groupby(\n",
    "    ['recipient_name', 'year'], as_index = False).agg({'grant_amount': sum})\n",
    "annualized_donations_df = pd.DataFrame(annualized_donations_df)\n",
    "# annualized_donations_df.to_excel(\"Output.xlsx\", index = False) # code to download as XSLX\n",
    "\n",
    "# Adding donations to text_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Organization_name\", \"year\": \"Document_year\"}) # Renaming group column\n",
    "regression_df[\"Organization_name\"] = regression_df[\"Organization_name\"].str.lower()\n",
    "regression_df = pd.merge(regression_df, annualized_donations_df, on = ['Organization_name', 'Document_year'], how = 'outer')\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(0)\n",
    "\n",
    "# Adding indicator for recieving a donation\n",
    "regression_df['recieved_donation'] = np.where(regression_df['grant_amount'] > 0, 1, 0)\n",
    "\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51bf214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating data for regressions\n",
    "logit_df = regression_df.copy()\n",
    "\n",
    "# Adding indicator for recieving a donation\n",
    "logit_df['experiment_group'] = np.where(\n",
    "    logit_df['Organization_name'].isin({\"greenpeace\", \"earthjustice\"}), 0, 1)\n",
    "\n",
    "# Dropping null values\n",
    "logit_df.dropna(inplace = True)\n",
    "\n",
    "# Exporting regression_df as xlsx\n",
    "logit_df.to_excel(\"logit_df.xlsx\")\n",
    "\n",
    "# Creating training and testing splits from logit_df\n",
    "y = logit_df['experiment_group'] # Either recieved_donation (y if got money that year or no) or experiment_group (either y or n)\n",
    "X = logit_df.drop(columns = [\"experiment_group\", \"recieved_donation\", \"grant_amount\", \"Organization_name\", \"Document_title\",\n",
    "                                  \"Document_type\", \"Reference\", \"Document_text\", \"Word_counts\",\n",
    "                                  \"cleaned_text\", \"Document_year\"])\n",
    "\n",
    "# Making sure all non-numeric columns and NaN values have been dropped\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "# Setting test size to 0.2 means that 80% of my data will be used to train and 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1680)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a35e3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit training data accuracy: 0.7551952500570907\n",
      "Logit testing data accuracy: 0.7461187214611872\n"
     ]
    }
   ],
   "source": [
    "### Logistic-LASSO Model\n",
    "logReg = LogisticRegressionCV(Cs = 20, cv = 10, penalty = 'l1', solver = 'liblinear',\n",
    "                              refit = True, class_weight = \"balanced\")\n",
    "logReg = logReg.fit(X_train, y_train)\n",
    "\n",
    "# Printing coefficient data\n",
    "coef = pd.DataFrame({'var':X.columns, 'val_lasso':logReg.coef_[0]})\n",
    "coef.sort_values(by = ['val_lasso'], inplace = True)\n",
    "coef.to_excel(\"logReg_coef.xlsx\", index = False)\n",
    "\n",
    "# Predicted probability for text recieving donation\n",
    "print(\"Logit training data accuracy: \" + str(accuracy_score(y_train, logReg.predict(X_train)))) # Prints accuracy score of y_train and Logit prediction of X_train\n",
    "print(\"Logit testing data accuracy: \" + str(accuracy_score(y_test, logReg.predict(X_test)))) # Prints accuracy score of y_test and Logit prediction of X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979cd97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
