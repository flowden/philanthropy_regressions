{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d1996bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data dictionaries\n",
    "\n",
    "# donations_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Grantmaker_name             String     Corporation/foundation that gave grant\n",
    "# Year                        Int        Year grant was given\n",
    "# Recipient_name              String     Organization that recived grant\n",
    "# NTEE_code                   String     NTEE code of organization given grant\n",
    "# NTEE_category               String     Broader category of organization according to IRS\n",
    "# Grant Amount                Float      Grant amount adjusted for inflation to 2020 dollars\n",
    "# Recipient_city              String     City of recipient organization\n",
    "# Recipient_state             String     State of recipient organization\n",
    "\n",
    "\n",
    "# text_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Group                       String     Name of environmental nonprofit\n",
    "# Individualism               Int        Measure of prevalence of this discourse of delay (DoD) in the text\n",
    "# The 'free rider' excuse     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Whataboutism                Int        Measure of prevalence of this DoD in the text in given year\n",
    "# All talk, little action     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Fossil fuel solutionism     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# No sticks, just carrots     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Technological optimism      Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to well-being        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Policy perfectionism        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to social justice    Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Change is impossible        Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Doomism                     Int        Measure of prevalence of this DoD in the text in given year\n",
    "# Year                        Int        Year associated with prevalence measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "905013a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/finnianlowden/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13cd86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing data\n",
    "\n",
    "# (1)\n",
    "# Corporate giving dataset - local\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# Dropping irrelevant columns (those not in data dictionary) from corporate philanthropy dataframe\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "# (2)\n",
    "# Text data\n",
    "# Adding experiment text data - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=570879331\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "experiment_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Adding control text data\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=1393581184\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "control_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Joining control and experiment\n",
    "complete_text_df = pd.concat([experiment_text_df, control_text_df])\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "text_df = complete_text_df.copy()\n",
    "text_df.drop(columns={'Researcher'}, inplace = True)\n",
    "\n",
    "\n",
    "# (3)\n",
    "# Discourses of Delay - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1MhB60vzde7KT9Ti6eQtimmWvYAEersI4zK3L_gwDNA8/edit#gid=0\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "delay_df = pd.read_csv(spreadsheet_url, header=0)\n",
    "\n",
    "simple_delay_names = {\"Individualism\": \"Individualism\", \"Whataboutism\":\n",
    "             \"Whataboutism\", \"Doomism\": \"Doomism\",\n",
    "             \"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots',\n",
    "             \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being',\n",
    "             \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice',\n",
    "             \"Change is impossible\": 'Change_impossible'}\n",
    "\n",
    "complete_discourse_dict = {}\n",
    "for row in delay_df.iterrows():\n",
    "    delay_method = row[1][\"Sub-category\"]\n",
    "    dict_words = row[1][\"Current_dict\"].split(\", \")\n",
    "    complete_discourse_dict[simple_delay_names[delay_method]] = dict_words\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "discourse_dict = complete_discourse_dict.copy()\n",
    "\n",
    "# (4)\n",
    "# Importing annualized_donations\n",
    "annualized_donations_df = pd.read_excel('annualized_donations.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2d2ec1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data wrangling\n",
    "\n",
    "# (1)\n",
    "# Text cleaning\n",
    "# Importing punctuation and stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™') \n",
    "\n",
    "# Cleaning text data\n",
    "textCleaned = [] # Creating an empty list to store list of cleaned words\n",
    "for row in text_df[\"Document_text\"]: # Looping through each Tweet in ukraineRussia_df\n",
    "    rowCleaned = [] # Creating an empty list to store cleaned words from each Tweet\n",
    "    row_as_list = str(row).split() # Splitting row into a list of words at ' '\n",
    "    for word in row_as_list: # Looping through each word in row_as_list\n",
    "        if word not in stopWords and word != \"nan\":\n",
    "            text = word.translate(table_punctuation) # Translating punctuation into ''\n",
    "            textLower = text.lower() # Converting text to lowercase\n",
    "            rowCleaned.append(textLower) # Appending cleaned word to rowCleaned list\n",
    "    textCleaned.append(rowCleaned)  # Appending rowCleaned to textCleaned list\n",
    "\n",
    "text_df[\"cleaned_text\"] = textCleaned\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating document-term matrix\n",
    "# Getting list of Discourses of Delay\n",
    "best_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\"]\n",
    "top6_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\", \"Free_rider\", \"Whataboutism\"]\n",
    "all_dicts = list(discourse_dict)\n",
    "delay_types = all_dicts\n",
    "\n",
    "# Getting all words in DoD dictionaries\n",
    "delay_vocabulary = set()\n",
    "for delay in delay_types:\n",
    "    delay_vocabulary.update(discourse_dict[delay])\n",
    "    \n",
    "regression_df = text_df.copy()\n",
    "    \n",
    "\n",
    "# Creating DoD_results dict\n",
    "DoD_results = {}\n",
    "    \n",
    "for col in delay_vocabulary:\n",
    "    wordAppearance = []\n",
    "    for text in text_df[\"cleaned_text\"]:\n",
    "        mySum = 0\n",
    "        prevWord = \"\"\n",
    "        for word in text:\n",
    "            if word == col:\n",
    "                mySum += 1\n",
    "            bigram = prevWord + \" \" + word\n",
    "            if bigram == col:\n",
    "                mySum += 1\n",
    "            # Creating DoD results dict\n",
    "            if word == col or bigram == col:\n",
    "                for delay in discourse_dict:\n",
    "                    og_words = [x.lower() for x in discourse_dict[delay]]\n",
    "                    if word in og_words:                \n",
    "                        if delay not in set(DoD_results):\n",
    "                            DoD_results[delay] = {word}\n",
    "                        else:\n",
    "                            DoD_results[delay].add(word)\n",
    "                    if bigram in og_words:                \n",
    "                        if delay not in set(DoD_results):\n",
    "                            DoD_results[delay] = {bigram}\n",
    "                        else:\n",
    "                            DoD_results[delay].add(bigram)    \n",
    "            prevWord = word  \n",
    "        wordAppearance.append(mySum)\n",
    "    if (sum(wordAppearance) > 0):\n",
    "        regression_df[col] = wordAppearance\n",
    "        regression_df = regression_df.copy()\n",
    "\n",
    "# Creating dict of words with their associated dictionaries      \n",
    "word_to_DoD = {}\n",
    "for delay in DoD_results:\n",
    "    wordSet = DoD_results[delay]\n",
    "    for word in wordSet:\n",
    "        if word not in word_to_DoD:\n",
    "            word_to_DoD[word] = {delay}\n",
    "        else:\n",
    "            word_to_DoD[word].add(delay)\n",
    "\n",
    "# (3)\n",
    "# Adding donation information\n",
    "# Adding donations to text_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Organization_name\", \"year\": \"Document_year\"}) # Renaming group column\n",
    "regression_df[\"Organization_name\"] = regression_df[\"Organization_name\"].str.lower()\n",
    "annualized_donations_df['Document_year'] -= 0\n",
    "regression_df = pd.merge(regression_df, annualized_donations_df, on = ['Organization_name', 'Document_year'], how = 'outer')\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lasso reg with dictionary counts as independent variables\n",
    "\n",
    "# Formatting data for Lasso regression\n",
    "lasso_df = regression_df.drop(columns={'Organization_name', 'Document_title', 'Document_type', 'Reference',\n",
    "                                      'Word_counts', 'cleaned_text', 'Document_year', 'Document_text'})\n",
    "\n",
    "# Dropping null values\n",
    "lasso_df.dropna(inplace = True)\n",
    "\n",
    "# Splitting data into X and Y\n",
    "y = lasso_df['grant_amount']\n",
    "X = lasso_df.drop(columns = {'grant_amount'})\n",
    "\n",
    "# Combining word counts into dictionary counts\n",
    "dict_sums = {} # Creating dict to store results\n",
    "\n",
    "# Removing words from discourse_dict\n",
    "wordsInText = list(lasso_df.drop(columns = {'grant_amount'}))\n",
    "discourse_dict_reg = discourse_dict.copy()\n",
    "\n",
    "for delay in discourse_dict_reg:\n",
    "    tempList = discourse_dict_reg[delay].copy() # Creating copy to not modify original list\n",
    "    for word in discourse_dict_reg[delay]:\n",
    "        if word not in wordsInText:\n",
    "            tempList.remove(word)\n",
    "    discourse_dict_reg[delay] = tempList # Adjusting pointer to copy\n",
    "    dict_sums[delay] = list(pd.DataFrame.sum(X[discourse_dict_reg[delay]], axis=1))\n",
    "    \n",
    "# Converting dictionary to DataFrame\n",
    "X = pd.DataFrame.from_dict(dict_sums)\n",
    "\n",
    "# Setting test size to 0.2 means that 80% of my data will be used to train and 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1680)\n",
    "\n",
    "# Creating variables to store alphas\n",
    "dictErrs = {}\n",
    "cv_errs = []\n",
    "coefs = []\n",
    "\n",
    "# Running a LASSO regression here\n",
    "potentialAlphas = np.linspace(1e-6, 1, num = 50).tolist()\n",
    "for alpha in potentialAlphas:\n",
    "    lassoReg = Lasso(alpha = alpha, normalize = True)\n",
    "    lassoReg.fit(X_train, y_train)\n",
    "    y_pred = lassoReg.predict(X_test)\n",
    "    cv_err = np.mean((y_pred - y_test)**2)\n",
    "    cv_errs.append(cv_err)\n",
    "    coefs.append(lassoReg.coef_)\n",
    "    dictErrs[cv_err] = alpha\n",
    "\n",
    "# Running LASSO with optimal alpha\n",
    "lassoReg = Lasso(alpha = dictErrs[min(cv_errs)], normalize = True) # running LASSO with best alpha\n",
    "lassoReg = lassoReg.fit(X_train, y_train)\n",
    "\n",
    "# Printing resuls from LASSOreg\n",
    "lasso_coef = pd.DataFrame({'var':X.columns, 'val_lasso':lassoReg.coef_})\n",
    "lasso_coef.sort_values(by='val_lasso', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a35e3fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>val_lasso</th>\n",
       "      <th>associated_dicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>best interest</td>\n",
       "      <td>452095.859928</td>\n",
       "      <td>Social_justice, Well_being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>near future</td>\n",
       "      <td>353123.280562</td>\n",
       "      <td>Tech_optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>exploiting</td>\n",
       "      <td>229907.837625</td>\n",
       "      <td>Free_rider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>prescribe</td>\n",
       "      <td>146623.525644</td>\n",
       "      <td>Carrots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>imminent</td>\n",
       "      <td>126918.426686</td>\n",
       "      <td>Tech_optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>lower carbon</td>\n",
       "      <td>-120635.522876</td>\n",
       "      <td>FF_solutionism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>fusion</td>\n",
       "      <td>-121346.252227</td>\n",
       "      <td>Tech_optimism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>carbon footprint</td>\n",
       "      <td>-160759.916169</td>\n",
       "      <td>Whataboutism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>mutually beneficial</td>\n",
       "      <td>-172851.847702</td>\n",
       "      <td>Carrots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>living standards</td>\n",
       "      <td>-173593.061203</td>\n",
       "      <td>Well_being</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    var      val_lasso            associated_dicts\n",
       "44        best interest  452095.859928  Social_justice, Well_being\n",
       "15          near future  353123.280562               Tech_optimism\n",
       "98           exploiting  229907.837625                  Free_rider\n",
       "24            prescribe  146623.525644                     Carrots\n",
       "59             imminent  126918.426686               Tech_optimism\n",
       "..                  ...            ...                         ...\n",
       "64         lower carbon -120635.522876              FF_solutionism\n",
       "65               fusion -121346.252227               Tech_optimism\n",
       "80     carbon footprint -160759.916169                Whataboutism\n",
       "39  mutually beneficial -172851.847702                     Carrots\n",
       "46     living standards -173593.061203                  Well_being\n",
       "\n",
       "[107 rows x 3 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lasso reg with words as independent variables\n",
    "\n",
    "# Formatting data for Lasso regression\n",
    "lasso_df = regression_df.drop(columns={'Organization_name', 'Document_title', 'Document_type', 'Reference',\n",
    "                                      'Word_counts', 'cleaned_text', 'Document_year', 'Document_text'})\n",
    "\n",
    "# Dropping null values\n",
    "lasso_df.dropna(inplace = True)\n",
    "\n",
    "# Splitting data into X and Y\n",
    "y = lasso_df['grant_amount']\n",
    "X = lasso_df.drop(columns = {'grant_amount'})\n",
    "\n",
    "# Setting test size to 0.2 means that 80% of my data will be used to train and 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1680)\n",
    "\n",
    "# Creating variables to store alphas\n",
    "dictErrs = {}\n",
    "cv_errs = []\n",
    "coefs = []\n",
    "\n",
    "# Running a LASSO regression here\n",
    "potentialAlphas = np.linspace(1e-6, 1, num = 50).tolist()\n",
    "for alpha in potentialAlphas:\n",
    "    lassoReg = Lasso(alpha = alpha, normalize = True)\n",
    "    lassoReg.fit(X_train, y_train)\n",
    "    y_pred = lassoReg.predict(X_test)\n",
    "    cv_err = np.mean((y_pred - y_test)**2)\n",
    "    cv_errs.append(cv_err)\n",
    "    coefs.append(lassoReg.coef_)\n",
    "    dictErrs[cv_err] = alpha\n",
    "\n",
    "# Running LASSO with optimal alpha\n",
    "lassoReg = Lasso(alpha = dictErrs[min(cv_errs)], normalize = True) # running LASSO with best alpha\n",
    "lassoReg = lassoReg.fit(X_train, y_train)\n",
    "\n",
    "# Printing resuls from LASSOreg\n",
    "lasso_coef = pd.DataFrame({'var':X.columns, 'val_lasso':lassoReg.coef_})\n",
    "dict_list = []\n",
    "for word in X.columns:\n",
    "    dict_list.append(str(word_to_DoD[word]).replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\"))\n",
    "lasso_coef['associated_dicts'] = dict_list\n",
    "lasso_coef.sort_values(by='val_lasso', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da0252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
