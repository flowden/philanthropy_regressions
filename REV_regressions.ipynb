{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60bc7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.api import OLS\n",
    "from math import sqrt\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b0e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing data\n",
    "\n",
    "# (1)\n",
    "# Corporate giving dataset - local\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# Dropping irrelevant columns (those not in data dictionary) from corporate philanthropy dataframe\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Text data - local\n",
    "text_df = pd.read_excel(\"ENVS_documents_for_text_analysis.xlsx\")\n",
    "text_df.drop(columns={'Researcher', 'Word_counts'}, inplace = True)\n",
    "\n",
    "\n",
    "# (3)\n",
    "# Discourses of Delay - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1MhB60vzde7KT9Ti6eQtimmWvYAEersI4zK3L_gwDNA8/edit#gid=0\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "delay_df = pd.read_csv(spreadsheet_url, header=0)\n",
    "\n",
    "simple_delay_names = {\"Individualism\": \"Individualism\", \"Whataboutism\":\n",
    "             \"Whataboutism\", \"Doomism\": \"Doomism\",\n",
    "             \"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots',\n",
    "             \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being',\n",
    "             \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice',\n",
    "             \"Change is impossible\": 'Change_impossible'}\n",
    "\n",
    "# Converting dataframe to dictionary with DoD words in list format\n",
    "complete_discourse_dict = {}\n",
    "for row in delay_df.iterrows():\n",
    "    delay_method = row[1][\"Sub-category\"]\n",
    "    dict_words = row[1][\"Current_dict\"].split(\", \")\n",
    "    complete_discourse_dict[simple_delay_names[delay_method]] = dict_words\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "discourse_dict = complete_discourse_dict.copy()\n",
    "\n",
    "# Pulling tech_optimism\n",
    "techOptimism = discourse_dict['Tech_optimism']\n",
    "\n",
    "\n",
    "# (4)\n",
    "# Importing annualized_donations - local\n",
    "annualized_donations_df = pd.read_excel('annualized_donations.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "765365fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data wrangling\n",
    "\n",
    "# (1)\n",
    "# Text cleaning\n",
    "# Importing punctuation and stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’') \n",
    "\n",
    "# Cleaning text data\n",
    "textCleaned = [] # Creating an empty list to store list of cleaned words\n",
    "for row in text_df[\"Document_text\"]: # Looping through each Tweet in ukraineRussia_df\n",
    "    rowCleaned = [] # Creating an empty list to store cleaned words from each Tweet\n",
    "    row_as_list = str(row).split() # Splitting row into a list of words at ' '\n",
    "    for word in row_as_list: # Looping through each word in row_as_list\n",
    "        if word not in stopWords and word != \"nan\":\n",
    "            text = word.translate(table_punctuation) # Translating punctuation into ''\n",
    "            textLower = text.lower() # Converting text to lowercase\n",
    "            rowCleaned.append(textLower) # Appending cleaned word to rowCleaned list\n",
    "    textCleaned.append(rowCleaned)  # Appending rowCleaned to textCleaned list\n",
    "text_df[\"cleaned_text\"] = textCleaned\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating document-term matrix\n",
    "delay_types = ['Tech_optimism']\n",
    "\n",
    "# Creating copy of text_df to work with\n",
    "regression_df = text_df.copy()\n",
    "    \n",
    "# Creating DoD_results dict\n",
    "DoD_results = {}\n",
    "\n",
    "wordsInText = []\n",
    "# Looping through each DoD word\n",
    "for col in techOptimism:\n",
    "    # Stemming col\n",
    "    col = stemmer.stem(col)\n",
    "    \n",
    "    wordAppearance = []\n",
    "    # Looping through each entry in text_df[\"cleaned_text\"]\n",
    "    for text in text_df[\"cleaned_text\"]:\n",
    "        mySum = 0\n",
    "        prevWord = \"\"\n",
    "        for word in text:\n",
    "            # Stemming word\n",
    "            word = stemmer.stem(word)\n",
    "            \n",
    "            # Incrementing if word in DoD vocabulary\n",
    "            if word == col:\n",
    "                mySum += 1\n",
    "            bigram = prevWord + \" \" + word\n",
    "            # Incrementing if bigram in DoD vocabulary\n",
    "            if bigram == col:\n",
    "                mySum += 1\n",
    "            prevWord = word  \n",
    "        wordAppearance.append(mySum)\n",
    "    # Adding word to regression_df if it appears in corpus\n",
    "    if (sum(wordAppearance) > 0):\n",
    "        regression_df[col] = wordAppearance\n",
    "        regression_df = regression_df.copy()\n",
    "        wordsInText.append(col)\n",
    "\n",
    "# (3)\n",
    "# Adding donation information to regression_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Organization_name\", \"year\": \"Document_year\"}) # Renaming group column\n",
    "regression_df[\"Organization_name\"] = regression_df[\"Organization_name\"].str.lower() # Converting name to lowercase\n",
    "regression_df = pd.merge(regression_df, annualized_donations_df, on = [\n",
    "    'Organization_name', 'Document_year'], how = 'outer') # Mergining on organization name and document year\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(0)\n",
    "\n",
    "\n",
    "# (4)\n",
    "# Computing word counts for each entry\n",
    "wordCounts = []\n",
    "for entry in regression_df[\"cleaned_text\"]:\n",
    "    wordCounts.append(len(str(entry).split()))\n",
    "regression_df[\"Word_counts\"] = wordCounts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f8f4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Formatting regression dataframe\n",
    "\n",
    "# (1)\n",
    "# Making frequency matrix\n",
    "wordCounts = regression_df[\"Word_counts\"]\n",
    "\n",
    "# Creating variables data\n",
    "variables = wordsInText.copy()\n",
    "variables = set(variables + ['grant_amount', 'Word_counts', 'Document_year', 'Organization_name'])\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating techOptimsm frequency measure\n",
    "reg_df = regression_df[variables]\n",
    "reg_df['techOpt_frequency'] = [0] * len(reg_df['grant_amount'])\n",
    "for word in wordsInText:\n",
    "    reg_df['techOpt_frequency'] += reg_df[word]\n",
    "reg_df['techOpt_intensity'] =   reg_df['techOpt_frequency']\n",
    "reg_df['techOpt_frequency'] = reg_df['techOpt_frequency'] / wordCounts * 100\n",
    "  \n",
    "# Dropping null values\n",
    "reg_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# (3)\n",
    "# Creating control data\n",
    "control_df = reg_df[reg_df['Organization_name'].isin(['greenpeace', 'earthjustice'])]\n",
    "controlGroupDict = {}\n",
    "for year in list(range(1980, 2021)):\n",
    "    controlGroupDict[year] = {'greenpeace': {\"count\": 0, \"length\": 0}, 'earthjustice': {\"count\": 0, \"length\": 0}}\n",
    "\n",
    "for row in control_df.iterrows():\n",
    "    group = row[1][\"Organization_name\"]\n",
    "    year = row[1][\"Document_year\"]\n",
    "    \n",
    "    # Updating dictionary\n",
    "    controlGroupDict[year][group][\"count\"] += row[1][\"techOpt_intensity\"]\n",
    "    controlGroupDict[year][group][\"length\"] += row[1][\"Word_counts\"]\n",
    "    \n",
    "# Calculating techOpt prevelance in control data\n",
    "controlDict = {}\n",
    "for year in controlGroupDict:\n",
    "    totalCount = 0\n",
    "    totalLength = 0\n",
    "    for group in controlGroupDict[year]:\n",
    "        totalCount += controlGroupDict[year][group][\"count\"]\n",
    "        totalLength += controlGroupDict[year][group][\"length\"]\n",
    "    if totalLength > 0:\n",
    "        controlDict[year] = totalCount / totalLength * 100\n",
    "    else:\n",
    "        controlDict[year] = 0\n",
    "\n",
    "# Using linear interpolation to fill in the gaps\n",
    "# Still significant even if removed\n",
    "controlDict[2001] = (controlDict[2000] + controlDict[2002])/2\n",
    "controlDict[2004] = (controlDict[2003]*2 + controlDict[2006])/3\n",
    "controlDict[2005] = (controlDict[2003] + controlDict[2006]*2)/3\n",
    "\n",
    "\n",
    "# (4)\n",
    "# Dropping greenpeace and earthjustice from reg_df\n",
    "reg_df = reg_df[~reg_df['Organization_name'].isin(['greenpeace', 'earthjustice'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d6f4e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:           grant_amount   R-squared (uncentered):                   0.135\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.135\n",
      "Method:                 Least Squares   F-statistic:                              246.3\n",
      "Date:                Mon, 16 May 2022   Prob (F-statistic):                   1.80e-148\n",
      "Time:                        10:55:10   Log-Likelihood:                         -67374.\n",
      "No. Observations:                4718   AIC:                                  1.348e+05\n",
      "Df Residuals:                    4715   BIC:                                  1.348e+05\n",
      "Df Model:                           3                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "year                    79.6511      4.594     17.339      0.000      70.645      88.657\n",
      "exp_techOpt_freq      7.014e+04   5452.599     12.864      0.000    5.95e+04    8.08e+04\n",
      "control_techOpt_freq -2.763e+05   3.72e+04     -7.426      0.000   -3.49e+05   -2.03e+05\n",
      "==============================================================================\n",
      "Omnibus:                     4292.652   Durbin-Watson:                   0.097\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           130053.402\n",
      "Skew:                           4.469   Prob(JB):                         0.00\n",
      "Kurtosis:                      27.118   Cond. No.                     1.33e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The condition number is large, 1.33e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "### Running regression\n",
    "\n",
    "# Splitting data into X and Y\n",
    "y = reg_df['grant_amount']\n",
    "X = reg_df[['Document_year', 'techOpt_frequency']]\n",
    "\n",
    "# Renaming columns\n",
    "X = X.rename(columns={'techOpt_frequency': 'exp_techOpt_freq', 'Document_year': 'year'})\n",
    "\n",
    "# Adding control data to dataframe\n",
    "correspondingControlList = []\n",
    "for year in X[\"year\"]:\n",
    "    correspondingControlList.append(controlDict[year])\n",
    "X['control_techOpt_freq'] = correspondingControlList\n",
    "\n",
    "# Creating linear regression\n",
    "olsReg = sm.OLS(y, X).fit()\n",
    "print(olsReg.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf06cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
