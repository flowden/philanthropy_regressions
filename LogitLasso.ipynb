{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda3a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/finn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import csv\n",
    "import nltk\n",
    "import statistics\n",
    "from nltk.corpus import stopwords # Importing stop words (e.g., the, and, a, of, etc.)\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1996bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data dictionaries\n",
    "\n",
    "\n",
    "# donations_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Grantmaker_name             String     Corporation/foundation that gave grant\n",
    "# Year                        Int        Year grant was given\n",
    "# Recipient_name              String     Organization that recived grant\n",
    "# NTEE_code                   String     NTEE code of organization given grant\n",
    "# NTEE_category               String     Broader category of organization according to IRS\n",
    "# Grant Amount                Float      Grant amount adjusted for inflation to 2020 dollars\n",
    "# Recipient_city              String     City of recipient organization\n",
    "# Recipient_state             String     State of recipient organization\n",
    "\n",
    "\n",
    "# text_df data dictionary\n",
    "# Variable                    Type       Description\n",
    "# Group                       String     Name of environmental nonprofit\n",
    "# Individualism               Float      Measure of prevalence of this discourse of delay (DoD) in the text\n",
    "# The 'free rider' excuse     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Whataboutism                Float      Measure of prevalence of this DoD in the text in given year\n",
    "# All talk, little action     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Fossil fuel solutionism     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# No sticks, just carrots     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Technological optimism      Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to well-being        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Policy perfectionism        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Appeal to social justice    Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Change is impossible        Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Doomism                     Float      Measure of prevalence of this DoD in the text in given year\n",
    "# Year                        Int        Year associated with prevalence measure\n",
    "# AF_indc                     Int        Indicator variable for American Forests (AF)\n",
    "# NFWF_indc                   Int        Indicator variable for the National Fish and Wildlife Foundation (NFWF)\n",
    "# NRDC_indc                   Int        Indicator variable for the Natural Resources Defense Council (NRDC)\n",
    "# CI_indc                     Int        Indicator variable for Conservation International(CI)\n",
    "# WWF_indc                    Int        Indicator variable for the World Wildlife Fund (WWF)\n",
    "# SC_indc                     Int        Indicator variable for the Sierra Club (SC)\n",
    "# OC_indc                     Int        Indicator variable for The Ocean Conservancy (OC)\n",
    "# EDF_indc                    Int        Indicator variable for the Environmental Defense Fund (EDF)\n",
    "# NAS_indc                    Int        Indicator variable for the National Audubon Society (NAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cd86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing data\n",
    "\n",
    "# (1)\n",
    "# Corporate giving dataset - local\n",
    "complete_donations_df = pd.read_excel(\"Oil_corporations_NTEE_Data_MASTER_SHEET.xlsx\", sheet_name = \"Individual_donations\")\n",
    "\n",
    "# (2)\n",
    "# Text data\n",
    "# Adding experiment text data - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=570879331\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "experiment_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Adding control text data\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1Zr4SQFxq8u3FnQwRyIHCoQ1Az_lb1PXd45Dhkni7Uok/edit#gid=1393581184\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\") # Online\n",
    "control_text_df = pd.read_csv(spreadsheet_url, header=0) # Online\n",
    "# Joining control and experiment\n",
    "complete_text_df = pd.concat([experiment_text_df, control_text_df])\n",
    "\n",
    "# (3)\n",
    "# Discourses of Delay - online\n",
    "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/1MhB60vzde7KT9Ti6eQtimmWvYAEersI4zK3L_gwDNA8/edit#gid=0\"\n",
    "spreadsheet_url = spreadsheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "delay_df = pd.read_csv(spreadsheet_url, header=0)\n",
    "\n",
    "simple_delay_names = {\"Individualism\": \"Individualism\", \"Whataboutism\":\n",
    "             \"Whataboutism\", \"Doomism\": \"Doomism\",\n",
    "             \"The 'free rider' excuse\": 'Free_rider',\n",
    "             \"All talk, little action\": 'Talk_no_action',\n",
    "             \"Fossil fuel solutionism\": 'FF_solutionism',\n",
    "             \"No sticks, just carrots\": 'Carrots',\n",
    "             \"Technological optimism\": 'Tech_optimism',\n",
    "             \"Appeal to well-being\": 'Well_being',\n",
    "             \"Policy perfectionism\": 'Perfect_policy',\n",
    "             \"Appeal to social justice\": 'Social_justice',\n",
    "             \"Change is impossible\": 'Change_impossible'}\n",
    "\n",
    "complete_discourse_dict = {}\n",
    "for row in delay_df.iterrows():\n",
    "    delay_method = row[1][\"Sub-category\"]\n",
    "    dict_words = row[1][\"Current_dict\"].split(\", \")\n",
    "    complete_discourse_dict[simple_delay_names[delay_method]] = dict_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b02c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Formatting dataframes\n",
    "\n",
    "# Working with corporate philanthropy data\n",
    "# Dropping irrelevant columns (those not in data dictionary)\n",
    "donations_df = complete_donations_df[[\"grantmaker_name\", \"year\", \"recipient_name\", \"NTEE_code\",\n",
    "                                      \"NTEE_category\", \"Grant Amount (2020 Dollars)\",\n",
    "                                      \"recipient_city\", \"recipient_state\"]]\n",
    "\n",
    "# Renaming Grant Amount (2020 Dollars) to not include spaces & converting to int\n",
    "donations_df = donations_df.rename(columns = {\"Grant Amount (2020 Dollars)\": \"grant_amount\"})\n",
    "donations_df[\"grant_amount\"] = donations_df[\"grant_amount\"]\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "text_df = complete_text_df.copy()\n",
    "text_df.drop(columns={'Researcher'}, inplace = True)\n",
    "\n",
    "# Making copy of complete_text_df\n",
    "discourse_dict = complete_discourse_dict.copy()\n",
    "\n",
    "# Checking to make sure changes were made\n",
    "# donations_df.head()\n",
    "# text_df.head()\n",
    "# discourse_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765bfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidating donations to orgs on an annual basis\n",
    "# Adding amount of donation in given year\n",
    "reduce_donations_df = donations_df.copy()\n",
    "group_list = ['nature conservancy', 'american forests', 'national fish and wildlife foundation',\n",
    " 'natural resources defense council', 'conservation international', 'world wildlife fund',\n",
    " 'sierra club', 'ocean conservancy', 'environmental defense fund', 'audubon society']\n",
    "reduce_donations_df[\"recipient_name\"] = reduce_donations_df[\"recipient_name\"].str.lower()\n",
    "boolean_series = reduce_donations_df[\"recipient_name\"].isin(group_list)\n",
    "reduce_donations_df = reduce_donations_df[boolean_series]\n",
    "\n",
    "# Grouping by year and group\n",
    "annualized_donations_df = reduce_donations_df.groupby(\n",
    "    ['recipient_name', 'year'], as_index = False).agg({'grant_amount': sum})\n",
    "annualized_donations_df = pd.DataFrame(annualized_donations_df)\n",
    "# annualized_donations_df.to_excel(\"Output.xlsx\", index = False) # code to download as XSLX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2ec1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF_solutionism {'low carbon', 'renewable natural', 'carbon intensity', 'natural gas', 'cleaner fuels', 'reliable energy', 'lower carbon'}\n",
      "Talk_no_action {'promises', 'commitment', 'promise', 'promised', 'committed', 'long term', 'commitments', 'ambition'}\n",
      "Carrots {'incentives', 'private sector', 'incentive', 'prescribe', 'mutually beneficial', 'volunteer', 'overburdened', 'burden', 'voluntary', 'marketplace'}\n",
      "Doomism {'inevitable', 'impossible', 'irreversible', 'resignation', 'catastrophe', 'grim', 'unequivocal', 'unavoidable', 'uncertainty', 'creator', 'fate', 'fear', 'extreme', 'adapt'}\n",
      "Individualism {'footprint', 'sacrifice', 'individual', 'consumer choice', 'personal'}\n",
      "Tech_optimism {'near future', 'energy management', 'sequestration', 'innovation', 'fusion', 'breakthrough', 'research development', 'invent', 'investment', 'invest', 'imminent', 'horizon'}\n",
      "Well_being {'best interest', 'disruptive', 'vulnerable', 'economic prosperity', 'consequence', 'lost', 'threat', 'living standards', 'energy needs'}\n",
      "Social_justice {'regressive', 'low income', 'unfair', 'inflation', 'best interest', 'costly', 'disproportionate', 'hinder', 'poor', 'strife', 'socioeconomic', 'affordable', 'burden', 'lowincome', 'disruption'}\n",
      "Change_impossible {'infeasible', 'adaptation', 'unimaginable', 'failure', 'insurmountable', 'doubt'}\n",
      "Perfect_policy {'bipartisan', 'nonpartisan', 'rush', 'cautious approach', 'compromise'}\n",
      "Whataboutism {'appetite', 'competition', 'target', 'carbon footprint', 'compete', 'negligible', 'total emissions'}\n",
      "Free_rider {'shared', 'exploited', 'share', 'take advantage', 'overburdened', 'burden', 'overused', 'overuse', 'tragedy commons', 'exploiting', 'exploit'}\n"
     ]
    }
   ],
   "source": [
    "### Data wrangling\n",
    "\n",
    "# (1)\n",
    "# Text cleaning\n",
    "# Importing punctuation and stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’') \n",
    "\n",
    "# Cleaning text data\n",
    "textCleaned = [] # Creating an empty list to store list of cleaned words\n",
    "for row in text_df[\"Document_text\"]: # Looping through each Tweet in ukraineRussia_df\n",
    "    rowCleaned = [] # Creating an empty list to store cleaned words from each Tweet\n",
    "    row_as_list = str(row).split() # Splitting row into a list of words at ' '\n",
    "    for word in row_as_list: # Looping through each word in row_as_list\n",
    "        if word not in stopWords and word != \"nan\":\n",
    "            text = word.translate(table_punctuation) # Translating punctuation into ''\n",
    "            textLower = text.lower() # Converting text to lowercase\n",
    "            rowCleaned.append(textLower) # Appending cleaned word to rowCleaned list\n",
    "    textCleaned.append(rowCleaned)  # Appending rowCleaned to textCleaned list\n",
    "\n",
    "text_df[\"cleaned_text\"] = textCleaned\n",
    "\n",
    "\n",
    "# (2)\n",
    "# Creating document-term matrix\n",
    "# Getting list of Discourses of Delay\n",
    "best_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\"]\n",
    "top6_dicts = [\"FF_solutionism\", \"Well_being\", \"Social_justice\", \"Carrots\", \"Free_rider\", \"Whataboutism\"]\n",
    "all_dicts = list(discourse_dict)\n",
    "delay_types = all_dicts\n",
    "\n",
    "# Getting all words in DoD dictionaries\n",
    "delay_vocabulary = set()\n",
    "for delay in delay_types:\n",
    "    delay_vocabulary.update(discourse_dict[delay])\n",
    "    \n",
    "regression_df = text_df.copy()\n",
    "    \n",
    "\n",
    "# Creating DoD_results dict\n",
    "DoD_results = {}\n",
    "    \n",
    "for col in delay_vocabulary:\n",
    "    wordAppearance = []\n",
    "    for text in text_df[\"cleaned_text\"]:\n",
    "        mySum = 0\n",
    "        prevWord = \"\"\n",
    "        for word in text:\n",
    "            if word == col:\n",
    "                mySum += 1\n",
    "            bigram = prevWord + \" \" + word\n",
    "            if bigram == col:\n",
    "                mySum += 1\n",
    "            # Creating DoD results dict\n",
    "            if word == col or bigram == col:\n",
    "                for delay in discourse_dict:\n",
    "                    og_words = [x.lower() for x in discourse_dict[delay]]\n",
    "                    if word in og_words:                \n",
    "                        if delay not in set(DoD_results):\n",
    "                            DoD_results[delay] = {word}\n",
    "                        else:\n",
    "                            DoD_results[delay].add(word)\n",
    "                    if bigram in og_words:                \n",
    "                        if delay not in set(DoD_results):\n",
    "                            DoD_results[delay] = {bigram}\n",
    "                        else:\n",
    "                            DoD_results[delay].add(bigram)    \n",
    "            prevWord = word  \n",
    "        wordAppearance.append(mySum)\n",
    "    if (sum(wordAppearance) > 0):\n",
    "        regression_df[col] = wordAppearance\n",
    "        regression_df = regression_df.copy()\n",
    "\n",
    "# Creating dict of words with their associated dictionaries      \n",
    "word_to_DoD = {}\n",
    "for delay in DoD_results:\n",
    "    wordSet = DoD_results[delay]\n",
    "    for word in wordSet:\n",
    "        if word not in word_to_DoD:\n",
    "            word_to_DoD[word] = {delay}\n",
    "        else:\n",
    "            word_to_DoD[word].add(delay)\n",
    "\n",
    "# (3)\n",
    "# Adding donation information\n",
    "# Adding donations to text_df\n",
    "annualized_donations_df = annualized_donations_df.rename(\n",
    "    columns = {\"recipient_name\": \"Organization_name\", \"year\": \"Document_year\"}) # Renaming group column\n",
    "regression_df[\"Organization_name\"] = regression_df[\"Organization_name\"].str.lower()\n",
    "annualized_donations_df['Document_year'] -= 1\n",
    "regression_df = pd.merge(regression_df, annualized_donations_df, on = ['Organization_name', 'Document_year'], how = 'outer')\n",
    "regression_df[\"grant_amount\"] = regression_df[\"grant_amount\"].fillna(0)\n",
    "\n",
    "# Adding indicator for recieving a donation\n",
    "regression_df['recieved_donation'] = np.where(regression_df['grant_amount'] > 0, 1, 0)\n",
    "\n",
    "# Helper function to link words with the DoD dicts they appear in\n",
    "def getDod(coef_df):\n",
    "    list_dicts = []\n",
    "    for word in coef_df[\"var\"]:\n",
    "        list_dicts.append(word_to_DoD[word])   \n",
    "    return list_dicts\n",
    "\n",
    "# Prints out each delay with words found in corpus vocabulary\n",
    "for delay in DoD_results:\n",
    "    print(delay, DoD_results[delay])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35e3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score predicting treatment w/ training data: 0.7551952500570907\n",
      "Accuracy score predicting treatment w/ testing data: 0.7461187214611872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>val_logitlasso</th>\n",
       "      <th>DoD_Dicts</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>failure</td>\n",
       "      <td>-0.443231</td>\n",
       "      <td>{Change_impossible}</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>promises</td>\n",
       "      <td>-0.442405</td>\n",
       "      <td>{Talk_no_action}</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>commitments</td>\n",
       "      <td>-0.442056</td>\n",
       "      <td>{Talk_no_action}</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>individual</td>\n",
       "      <td>-0.388766</td>\n",
       "      <td>{Individualism}</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>committed</td>\n",
       "      <td>-0.269252</td>\n",
       "      <td>{Talk_no_action}</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>innovation</td>\n",
       "      <td>0.233737</td>\n",
       "      <td>{Tech_optimism}</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>horizon</td>\n",
       "      <td>0.364421</td>\n",
       "      <td>{Tech_optimism}</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>volunteer</td>\n",
       "      <td>0.485640</td>\n",
       "      <td>{Carrots}</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>bipartisan</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>{Perfect_policy}</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>incentives</td>\n",
       "      <td>1.207541</td>\n",
       "      <td>{Carrots}</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             var  val_logitlasso            DoD_Dicts  wordCount\n",
       "0        failure       -0.443231  {Change_impossible}       82.0\n",
       "1       promises       -0.442405     {Talk_no_action}       39.0\n",
       "2    commitments       -0.442056     {Talk_no_action}      190.0\n",
       "3     individual       -0.388766      {Individualism}      261.0\n",
       "4      committed       -0.269252     {Talk_no_action}      393.0\n",
       "..           ...             ...                  ...        ...\n",
       "100   innovation        0.233737      {Tech_optimism}      188.0\n",
       "101      horizon        0.364421      {Tech_optimism}       99.0\n",
       "102    volunteer        0.485640            {Carrots}      304.0\n",
       "103   bipartisan        0.607850     {Perfect_policy}      107.0\n",
       "104   incentives        1.207541            {Carrots}      227.0\n",
       "\n",
       "[105 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LogitLasso reg\n",
    "\n",
    "# Predicting treatment vs control\n",
    "# Creating data for regressions\n",
    "logit_df = regression_df.copy()\n",
    "regression_df.to_excel(\"regression_df.xlsx\")\n",
    "\n",
    "# Adding indicator for recieving a donation\n",
    "logit_df['experiment_group'] = np.where(\n",
    "    logit_df['Organization_name'].isin({\"greenpeace\", \"earthjustice\"}), 0, 1)\n",
    "\n",
    "# Dropping null values\n",
    "logit_df.dropna(inplace = True)\n",
    "\n",
    "# Exporting logit_df as xlsx\n",
    "logit_df.to_excel(\"logit_df.xlsx\")\n",
    "\n",
    "# Creating training and testing splits from logit_df\n",
    "y = logit_df['experiment_group']\n",
    "X = logit_df.drop(columns = [\"experiment_group\", \"recieved_donation\", \"grant_amount\", \"Organization_name\", \"Document_title\",\n",
    "                                  \"Document_type\", \"Reference\", \"Document_text\", \"Word_counts\",\n",
    "                                  \"cleaned_text\", \"Document_year\"])\n",
    "\n",
    "# Making sure all non-numeric columns and NaN values have been dropped\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "# Setting test size to 0.2 means that 80% of my data will be used to train and 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1680)\n",
    "\n",
    "### Logistic-LASSO Model\n",
    "logReg = LogisticRegressionCV(Cs = 20, cv = 10, penalty = 'l1', solver = 'liblinear',\n",
    "                              refit = True, class_weight = \"balanced\")\n",
    "logReg = logReg.fit(X_train, y_train)\n",
    "\n",
    "# Printing coefficient data\n",
    "coef1 = pd.DataFrame({'var':X.columns, 'val_logitlasso':logReg.coef_[0]})\n",
    "coef1.sort_values(by = ['val_logitlasso'], inplace = True)\n",
    "coef1[\"DoD_Dicts\"] = getDod(coef1)\n",
    "\n",
    "# Adding word counts\n",
    "wordList = []\n",
    "sumList = []\n",
    "for word in X:\n",
    "    wordList.append(word)\n",
    "    sumList.append(X[word].sum())\n",
    "wordCounts_df = pd.DataFrame(list(zip(wordList, sumList)), columns=[\"var\", \"wordCount\"])\n",
    "coef1 = pd.merge(coef1, wordCounts_df, on = ['var'], how = 'left')\n",
    "\n",
    "# Saving as xlsx\n",
    "coef1.to_excel(\"logReg_coef_expGroup.xlsx\", index = False)\n",
    "\n",
    "# Predicted probability for text recieving donation\n",
    "print(\"Accuracy score predicting treatment w/ training data: \" + str(\n",
    "    accuracy_score(y_train, logReg.predict(X_train)))) # Prints accuracy score of y_train and Logit prediction of X_train\n",
    "print(\"Accuracy score predicting treatment w/ testing data: \" + str(\n",
    "    accuracy_score(y_test, logReg.predict(X_test)))) # Prints accuracy score of y_test and Logit prediction of X_train\n",
    "coef1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979cd97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score predicting recieved_donation w/ training data: 0.6204612925325417\n",
      "Accuracy score predicting recieved_donation w/ testing data: 0.617351598173516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>val_logitlasso</th>\n",
       "      <th>DoD_Dicts</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low carbon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{FF_solutionism}</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unimaginable</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Change_impossible}</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploited</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Free_rider}</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reliable energy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{FF_solutionism}</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>strife</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Social_justice}</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>infeasible</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Change_impossible}</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>unequivocal</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Doomism}</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>costly</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Social_justice}</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>burden</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Free_rider, Social_justice, Carrots}</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>exploiting</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{Free_rider}</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 var  val_logitlasso                              DoD_Dicts  \\\n",
       "0         low carbon             0.0                       {FF_solutionism}   \n",
       "1       unimaginable             0.0                    {Change_impossible}   \n",
       "2          exploited             0.0                           {Free_rider}   \n",
       "3    reliable energy             0.0                       {FF_solutionism}   \n",
       "4             strife             0.0                       {Social_justice}   \n",
       "..               ...             ...                                    ...   \n",
       "100       infeasible             0.0                    {Change_impossible}   \n",
       "101      unequivocal             0.0                              {Doomism}   \n",
       "102           costly             0.0                       {Social_justice}   \n",
       "103           burden             0.0  {Free_rider, Social_justice, Carrots}   \n",
       "104       exploiting             0.0                           {Free_rider}   \n",
       "\n",
       "     wordCount  \n",
       "0          6.0  \n",
       "1          4.0  \n",
       "2         16.0  \n",
       "3          1.0  \n",
       "4          3.0  \n",
       "..         ...  \n",
       "100        3.0  \n",
       "101        6.0  \n",
       "102       49.0  \n",
       "103       35.0  \n",
       "104       11.0  \n",
       "\n",
       "[105 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LogitLasso reg\n",
    "\n",
    "# Predicting if money was recieved that year\n",
    "# Creating data for regressions\n",
    "logit_df = regression_df.copy()\n",
    "\n",
    "# Dropping null values\n",
    "logit_df.dropna(inplace = True)\n",
    "\n",
    "# Exporting regression_df as xlsx\n",
    "logit_df.to_excel(\"logit_df.xlsx\")\n",
    "\n",
    "# Creating training and testing splits from logit_df\n",
    "y = logit_df['recieved_donation']\n",
    "X = logit_df.drop(columns = [\"recieved_donation\", \"grant_amount\", \"Organization_name\", \"Document_title\",\n",
    "                                  \"Document_type\", \"Reference\", \"Document_text\", \"Word_counts\",\n",
    "                                  \"cleaned_text\", \"Document_year\"])\n",
    "\n",
    "# Making sure all non-numeric columns and NaN values have been dropped\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "# Setting test size to 0.2 means that 80% of my data will be used to train and 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1680)\n",
    "\n",
    "### Logistic-LASSO Model\n",
    "logReg = LogisticRegressionCV(Cs = 20, cv = 10, penalty = 'l1', solver = 'liblinear', refit = True)\n",
    "logReg = logReg.fit(X_train, y_train)\n",
    "\n",
    "# Printing coefficient data\n",
    "coef2 = pd.DataFrame({'var':X.columns, 'val_logitlasso':logReg.coef_[0]})\n",
    "coef2.sort_values(by = ['val_logitlasso'], inplace = True)\n",
    "coef2[\"DoD_Dicts\"] = getDod(coef2)\n",
    "\n",
    "# Adding word counts\n",
    "wordList = []\n",
    "sumList = []\n",
    "for word in X:\n",
    "    wordList.append(word)\n",
    "    sumList.append(X[word].sum())\n",
    "wordCounts_df = pd.DataFrame(list(zip(wordList, sumList)), columns=[\"var\", \"wordCount\"])\n",
    "coef2 = pd.merge(coef2, wordCounts_df, on = ['var'], how = 'left')\n",
    "\n",
    "coef2.to_excel(\"logReg_coef_gotMoneyYear.xlsx\", index = False)\n",
    "\n",
    "# Predicted probability for text recieving donation\n",
    "print(\"Accuracy score predicting recieved_donation w/ training data: \" + str(\n",
    "    accuracy_score(y_train, logReg.predict(X_train)))) # Prints accuracy score of y_train and Logit prediction of X_train\n",
    "print(\"Accuracy score predicting recieved_donation w/ testing data: \" + str(\n",
    "    accuracy_score(y_test, logReg.predict(X_test)))) # Prints accuracy score of y_test and Logit prediction of X_train\n",
    "coef2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32041a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
